{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016942,
     "end_time": "2022-12-09T01:29:12.309116",
     "exception": false,
     "start_time": "2022-12-09T01:29:12.292174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The goal of this kernel is to train a simple transformer from only pytorch to classify hate speech in comments\n",
    "to understand how the transformer works, I would recommend this video it walks through the Attention paper and explains it well\n",
    "\n",
    "https://www.youtube.com/watch?v=U0s0f995w14&t=2522s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014437,
     "end_time": "2022-12-09T01:29:12.338351",
     "exception": false,
     "start_time": "2022-12-09T01:29:12.323914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I will be using  torchtext it's really good for preparing the data, it has a good documentation also \n",
    "those are links about an example using torchtext and a tutorial\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
    "\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01431,
     "end_time": "2022-12-09T01:29:12.367184",
     "exception": false,
     "start_time": "2022-12-09T01:29:12.352874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "as this will be a classification task, we will just need the encoder part of the transformer, so I used the transformer encoder layer from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:12.405005Z",
     "iopub.status.busy": "2022-12-09T01:29:12.404282Z",
     "iopub.status.idle": "2022-12-09T01:29:14.878985Z",
     "shell.execute_reply": "2022-12-09T01:29:14.878521Z"
    },
    "id": "cj9NXWy3zd4T",
    "outputId": "0d72ee04-eea7-42f6-e1f0-d7fccc461a9a",
    "papermill": {
     "duration": 2.497437,
     "end_time": "2022-12-09T01:29:14.879099",
     "exception": false,
     "start_time": "2022-12-09T01:29:12.381662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu up\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 32\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import  f1_score\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.nn  import functional as F\n",
    "import torch.optim as  optim \n",
    "import dill as dill\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "  print(\"gpu up\")\n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:14.913889Z",
     "iopub.status.busy": "2022-12-09T01:29:14.913302Z",
     "iopub.status.idle": "2022-12-09T01:29:14.916960Z",
     "shell.execute_reply": "2022-12-09T01:29:14.916546Z"
    },
    "papermill": {
     "duration": 0.022382,
     "end_time": "2022-12-09T01:29:14.917067",
     "exception": false,
     "start_time": "2022-12-09T01:29:14.894685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add dataset\n",
    "# https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:14.955971Z",
     "iopub.status.busy": "2022-12-09T01:29:14.955069Z",
     "iopub.status.idle": "2022-12-09T01:29:14.964405Z",
     "shell.execute_reply": "2022-12-09T01:29:14.964815Z"
    },
    "papermill": {
     "duration": 0.032442,
     "end_time": "2022-12-09T01:29:14.964930",
     "exception": false,
     "start_time": "2022-12-09T01:29:14.932488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:14.999773Z",
     "iopub.status.busy": "2022-12-09T01:29:14.999108Z",
     "iopub.status.idle": "2022-12-09T01:29:15.119428Z",
     "shell.execute_reply": "2022-12-09T01:29:15.120457Z"
    },
    "papermill": {
     "duration": 0.140462,
     "end_time": "2022-12-09T01:29:15.120635",
     "exception": false,
     "start_time": "2022-12-09T01:29:14.980173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#breaking data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labeled_dataframe = pd.read_csv('/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv')\n",
    "labeled_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:15.176268Z",
     "iopub.status.busy": "2022-12-09T01:29:15.175745Z",
     "iopub.status.idle": "2022-12-09T01:29:15.183475Z",
     "shell.execute_reply": "2022-12-09T01:29:15.182900Z"
    },
    "papermill": {
     "duration": 0.037793,
     "end_time": "2022-12-09T01:29:15.183625",
     "exception": false,
     "start_time": "2022-12-09T01:29:15.145832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split \n",
    "from pathlib import Path\n",
    "traindata, test = train_test_split(labeled_dataframe, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:15.224724Z",
     "iopub.status.busy": "2022-12-09T01:29:15.224033Z",
     "iopub.status.idle": "2022-12-09T01:29:17.225408Z",
     "shell.execute_reply": "2022-12-09T01:29:17.226136Z"
    },
    "id": "k5O16FLBzhuC",
    "outputId": "6f53909e-a92b-46c6-ed25-95a67abfc8ca",
    "papermill": {
     "duration": 2.025628,
     "end_time": "2022-12-09T01:29:17.226350",
     "exception": false,
     "start_time": "2022-12-09T01:29:15.200722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "those are the libraries I use for processing text\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem  import PorterStemmer\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def removepunc(my_str): # function to remove punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "snowstem = SnowballStemmer(\"english\")\n",
    "portstem = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:17.280474Z",
     "iopub.status.busy": "2022-12-09T01:29:17.279611Z",
     "iopub.status.idle": "2022-12-09T01:29:17.288016Z",
     "shell.execute_reply": "2022-12-09T01:29:17.288498Z"
    },
    "id": "elXvdWQyzkDw",
    "papermill": {
     "duration": 0.040245,
     "end_time": "2022-12-09T01:29:17.288621",
     "exception": false,
     "start_time": "2022-12-09T01:29:17.248376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "#traindata = toxic_train_df #pd.read_csv(\"/kaggle/input/hate-speech-detection/toxic_train.csv\")\n",
    "#test = toxic_test_df #pd.read_csv(\"/kaggle/input/hate-speech-detection/toxic_test.csv\")\n",
    "traindata.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "test.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:17.333643Z",
     "iopub.status.busy": "2022-12-09T01:29:17.332868Z",
     "iopub.status.idle": "2022-12-09T01:29:17.336537Z",
     "shell.execute_reply": "2022-12-09T01:29:17.336927Z"
    },
    "id": "lTDXODNhzwwz",
    "papermill": {
     "duration": 0.031066,
     "end_time": "2022-12-09T01:29:17.337051",
     "exception": false,
     "start_time": "2022-12-09T01:29:17.305985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function is the tokenizer we are using, it does basic processing also  like ,\n",
    "Lowercase the text\n",
    "removing punctuation, stop words and numbers,\n",
    "it also removes extra spaces and unwanted characters (I use regex for that)\n",
    "\n",
    "\n",
    "before using the tokenizer I was testing it on the train dataframe manually  \n",
    "\"\"\"\n",
    "\n",
    "def myTokenizer(x):\n",
    " return  [snowstem.stem(word.text)for word in \n",
    "          tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?@\\'\\`\\\"\\r+\\r+\\n+\\n+\\b+]\",\" \",x.lower()))).strip()) \n",
    "          if (word.text not in stops and not hasNumbers(word.text)) ]\n",
    "\n",
    "# myTokenizer = data.get_tokenizer(\"basic_english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:17.386081Z",
     "iopub.status.busy": "2022-12-09T01:29:17.384954Z",
     "iopub.status.idle": "2022-12-09T01:29:28.932102Z",
     "shell.execute_reply": "2022-12-09T01:29:28.931240Z"
    },
    "id": "udmV7yOmPNt6",
    "papermill": {
     "duration": 11.575801,
     "end_time": "2022-12-09T01:29:28.932214",
     "exception": false,
     "start_time": "2022-12-09T01:29:17.356413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here I'm using the torchtext fields and dataset classes they can ease the work to get\n",
    "the dataset ready for the pytorch model\n",
    "\n",
    "the class DataFrameDataset is the easiest way I found to turn a dataframe into a torchtext dataset\n",
    "\n",
    "this cell will take sometime to finish\n",
    "\"\"\"\n",
    "\n",
    "TEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\n",
    "LABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n",
    "\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('comment_text', text_field), ('toxic', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            if row['class'] == 2:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "                \n",
    "            text = row['tweet']\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "  \n",
    "\n",
    "torchdataset = DataFrameDataset(traindata, TEXT,LABEL)\n",
    "torchtest = DataFrameDataset(test, TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:28.985782Z",
     "iopub.status.busy": "2022-12-09T01:29:28.975564Z",
     "iopub.status.idle": "2022-12-09T01:29:28.996727Z",
     "shell.execute_reply": "2022-12-09T01:29:28.995888Z"
    },
    "id": "-f1hgGywizQT",
    "papermill": {
     "duration": 0.046547,
     "end_time": "2022-12-09T01:29:28.996826",
     "exception": false,
     "start_time": "2022-12-09T01:29:28.950279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:29.086056Z",
     "iopub.status.busy": "2022-12-09T01:29:29.070908Z",
     "iopub.status.idle": "2022-12-09T01:29:29.305784Z",
     "shell.execute_reply": "2022-12-09T01:29:29.305258Z"
    },
    "id": "O6PUzivRJvL_",
    "papermill": {
     "duration": 0.290997,
     "end_time": "2022-12-09T01:29:29.305902",
     "exception": false,
     "start_time": "2022-12-09T01:29:29.014905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this cell build the vocab which means it get all the used words and if also ignores any word \n",
    "that only appeared less than 3 times\n",
    "\"\"\"\n",
    "TEXT.build_vocab(train_data,min_freq=3)  \n",
    "LABEL.build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:29.352771Z",
     "iopub.status.busy": "2022-12-09T01:29:29.351930Z",
     "iopub.status.idle": "2022-12-09T01:29:29.357157Z",
     "shell.execute_reply": "2022-12-09T01:29:29.356638Z"
    },
    "id": "QojEJaoBVTJj",
    "outputId": "528174ce-a162-47bf-edb4-ab3358bf296a",
    "papermill": {
     "duration": 0.033043,
     "end_time": "2022-12-09T01:29:29.357254",
     "exception": false,
     "start_time": "2022-12-09T01:29:29.324211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 4900\n",
      "Size of LABEL vocabulary: 2\n",
      "[('bitch', 7336), ('rt', 4843), ('hoe', 2793), ('co', 1901), ('http', 1834), ('like', 1817), ('pussi', 1427), ('fuck', 1411), ('im', 1334), ('nigga', 1274)]\n",
      "4900\n"
     ]
    }
   ],
   "source": [
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "print(len(TEXT.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:29.399173Z",
     "iopub.status.busy": "2022-12-09T01:29:29.398608Z",
     "iopub.status.idle": "2022-12-09T01:29:29.402052Z",
     "shell.execute_reply": "2022-12-09T01:29:29.401607Z"
    },
    "id": "JVrwFrmTqHzc",
    "papermill": {
     "duration": 0.026538,
     "end_time": "2022-12-09T01:29:29.402147",
     "exception": false,
     "start_time": "2022-12-09T01:29:29.375609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\"\"\"\n",
    "we are using batches for validation and test set because of memory usage we can't pass the whole set at once \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n",
    "    (train_data,valid_data,torchtest), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort =False,\n",
    "shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:29.450819Z",
     "iopub.status.busy": "2022-12-09T01:29:29.450130Z",
     "iopub.status.idle": "2022-12-09T01:29:34.180761Z",
     "shell.execute_reply": "2022-12-09T01:29:34.181187Z"
    },
    "id": "QA57LLmjMCX3",
    "outputId": "964eb82a-0855-4675-a5c7-7ae4f6859d1e",
    "papermill": {
     "duration": 4.760785,
     "end_time": "2022-12-09T01:29:34.181345",
     "exception": false,
     "start_time": "2022-12-09T01:29:29.420560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextTransformer(\n",
       "  (wordEmbeddings): Embedding(4900, 140)\n",
       "  (positionEmbeddings): Embedding(140, 20)\n",
       "  (transformerLayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=160, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=160, bias=True)\n",
       "    (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (linear1): Linear(in_features=160, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (linear3): Linear(in_features=140, out_features=16, bias=True)\n",
       "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "one major point here is that I encoded the embeddings in a different way \n",
    "I made an embedding layer for the position then I concatenated position embeddings with the word embeddings \n",
    "just thought it could be a usefull way to encode the positions \n",
    "\n",
    "had to reshape the output of the transformer layer to get the prediction\n",
    "\"\"\"\n",
    "class TextTransformer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextTransformer,self).__init__()\n",
    "    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n",
    "    self.positionEmbeddings = nn.Embedding(140,20)\n",
    "    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n",
    "    self.linear1 = nn.Linear(160,  64)\n",
    "    self.linear2 = nn.Linear(64,  1)\n",
    "    self.linear3 = nn.Linear(140,  16)\n",
    "    self.linear4 = nn.Linear(16,  1)\n",
    "  def forward(self,x):\n",
    "    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n",
    "    # broadcasting the tensor of positions \n",
    "    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n",
    "    attended = self.transformerLayer(sentence)\n",
    "    linear1 = F.relu(self.linear1(attended))\n",
    "    linear2 = F.relu(self.linear2(linear1))\n",
    "    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
    "    linear3 = F.relu(self.linear3(linear2))\n",
    "    out = torch.sigmoid(self.linear4(linear3))\n",
    "    return out\n",
    "\n",
    "myTransformer = TextTransformer()\n",
    "myTransformer.to(device)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:34.225491Z",
     "iopub.status.busy": "2022-12-09T01:29:34.224659Z",
     "iopub.status.idle": "2022-12-09T01:29:34.227293Z",
     "shell.execute_reply": "2022-12-09T01:29:34.226783Z"
    },
    "id": "OQOBpRBUAyAk",
    "papermill": {
     "duration": 0.026228,
     "end_time": "2022-12-09T01:29:34.227396",
     "exception": false,
     "start_time": "2022-12-09T01:29:34.201168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculateMetrics(ypred,ytrue):\n",
    "  acc  = accuracy_score(ytrue,ypred)\n",
    "  f1  = f1_score(ytrue,ypred)\n",
    "  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n",
    "  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:29:34.277031Z",
     "iopub.status.busy": "2022-12-09T01:29:34.276396Z",
     "iopub.status.idle": "2022-12-09T01:37:15.667611Z",
     "shell.execute_reply": "2022-12-09T01:37:15.666925Z"
    },
    "id": "zbLZTzgxJRIA",
    "outputId": "21fbc41e-a284-4267-d3ca-0c10387db2a7",
    "papermill": {
     "duration": 461.421792,
     "end_time": "2022-12-09T01:37:15.667776",
     "exception": false,
     "start_time": "2022-12-09T01:29:34.245984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BCE loss:  0.6324504017829895  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6270715594291687  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6260674595832825  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6223059892654419  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6219716668128967  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.618550181388855  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6185860633850098  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6153443455696106  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6156609654426575  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6125164031982422  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6130160093307495  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6099120378494263  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6105647683143616  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6074807643890381  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6082632541656494  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6051873564720154  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.606084406375885  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.603007972240448  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6040069460868835  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.6009227633476257  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.6020174026489258  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5989220142364502  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.600103497505188  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5969917178153992  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5982559323310852  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5951265096664429  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5964677929878235  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5933197736740112  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5947333574295044  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5915638208389282  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5930470824241638  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5898535251617432  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5914051532745361  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5881896018981934  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5898045897483826  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.586564838886261  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5882429480552673  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5849771499633789  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5867154002189636  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5834237337112427  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5852212309837341  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5819048881530762  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5837591290473938  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5804169178009033  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5823250412940979  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5789566040039062  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5809201598167419  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5775232911109924  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5795401334762573  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5761170983314514  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5781862735748291  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5747353434562683  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5768551826477051  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5733783841133118  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5755463242530823  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.572037935256958  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5742572546005249  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5707237124443054  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5729897022247314  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5694276690483093  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5717412233352661  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5681521892547607  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5705115795135498  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5668917894363403  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.5686926245689392  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.5746363401412964  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.45487481355667114  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.4381001591682434  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.4427063465118408  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.4288509786128998  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.4294319152832031  f1 score: 0.0 f1 average: 0.453 accuracy: 0.83\n",
      "validation BCE loss:  0.41686731576919556  f1 score: 0.0 f1 average: 0.455 accuracy: 0.836\n",
      "train BCE loss:  0.4134930372238159  f1 score: 0.007 f1 average: 0.457 accuracy: 0.83\n",
      "validation BCE loss:  0.4051283597946167  f1 score: 0.009 f1 average: 0.46 accuracy: 0.837\n",
      "train BCE loss:  0.39692720770835876  f1 score: 0.071 f1 average: 0.49 accuracy: 0.834\n",
      "validation BCE loss:  0.3948212265968323  f1 score: 0.056 f1 average: 0.484 accuracy: 0.839\n",
      "train BCE loss:  0.38298341631889343  f1 score: 0.174 f1 average: 0.543 accuracy: 0.841\n",
      "validation BCE loss:  0.38716933131217957  f1 score: 0.11 f1 average: 0.511 accuracy: 0.841\n",
      "train BCE loss:  0.3706887364387512  f1 score: 0.252 f1 average: 0.583 accuracy: 0.847\n",
      "validation BCE loss:  0.37960267066955566  f1 score: 0.161 f1 average: 0.538 accuracy: 0.845\n",
      "train BCE loss:  0.36134520173072815  f1 score: 0.314 f1 average: 0.616 accuracy: 0.852\n",
      "validation BCE loss:  0.3761585056781769  f1 score: 0.22 f1 average: 0.568 accuracy: 0.849\n",
      "train BCE loss:  0.35268083214759827  f1 score: 0.368 f1 average: 0.644 accuracy: 0.858\n",
      "validation BCE loss:  0.37016767263412476  f1 score: 0.24 f1 average: 0.578 accuracy: 0.85\n",
      "train BCE loss:  0.3439663052558899  f1 score: 0.403 f1 average: 0.663 accuracy: 0.862\n",
      "validation BCE loss:  0.36579734086990356  f1 score: 0.281 f1 average: 0.6 accuracy: 0.854\n",
      "train BCE loss:  0.33608436584472656  f1 score: 0.432 f1 average: 0.678 accuracy: 0.865\n",
      "validation BCE loss:  0.36075371503829956  f1 score: 0.319 f1 average: 0.62 accuracy: 0.858\n",
      "train BCE loss:  0.3284986615180969  f1 score: 0.469 f1 average: 0.698 accuracy: 0.871\n",
      "validation BCE loss:  0.3564155399799347  f1 score: 0.329 f1 average: 0.625 accuracy: 0.858\n",
      "train BCE loss:  0.3212980628013611  f1 score: 0.493 f1 average: 0.711 accuracy: 0.875\n",
      "validation BCE loss:  0.3523200750350952  f1 score: 0.332 f1 average: 0.626 accuracy: 0.858\n",
      "train BCE loss:  0.3151494562625885  f1 score: 0.511 f1 average: 0.721 accuracy: 0.878\n",
      "validation BCE loss:  0.349904328584671  f1 score: 0.363 f1 average: 0.642 accuracy: 0.86\n",
      "train BCE loss:  0.3075272738933563  f1 score: 0.542 f1 average: 0.737 accuracy: 0.882\n",
      "validation BCE loss:  0.34455570578575134  f1 score: 0.39 f1 average: 0.657 accuracy: 0.864\n",
      "train BCE loss:  0.30162426829338074  f1 score: 0.545 f1 average: 0.739 accuracy: 0.883\n",
      "validation BCE loss:  0.3432847559452057  f1 score: 0.387 f1 average: 0.655 accuracy: 0.864\n",
      "train BCE loss:  0.2957180142402649  f1 score: 0.57 f1 average: 0.753 accuracy: 0.888\n",
      "validation BCE loss:  0.34276026487350464  f1 score: 0.399 f1 average: 0.662 accuracy: 0.866\n",
      "train BCE loss:  0.28948718309402466  f1 score: 0.585 f1 average: 0.761 accuracy: 0.89\n",
      "validation BCE loss:  0.3378066420555115  f1 score: 0.422 f1 average: 0.674 accuracy: 0.869\n",
      "train BCE loss:  0.28578561544418335  f1 score: 0.594 f1 average: 0.765 accuracy: 0.891\n",
      "validation BCE loss:  0.3375091850757599  f1 score: 0.436 f1 average: 0.681 accuracy: 0.87\n",
      "train BCE loss:  0.27968165278434753  f1 score: 0.606 f1 average: 0.772 accuracy: 0.894\n",
      "validation BCE loss:  0.3359012007713318  f1 score: 0.443 f1 average: 0.685 accuracy: 0.871\n",
      "train BCE loss:  0.2753344774246216  f1 score: 0.62 f1 average: 0.78 accuracy: 0.897\n",
      "validation BCE loss:  0.3329653739929199  f1 score: 0.464 f1 average: 0.696 accuracy: 0.872\n",
      "train BCE loss:  0.2707115709781647  f1 score: 0.627 f1 average: 0.784 accuracy: 0.898\n",
      "validation BCE loss:  0.33138421177864075  f1 score: 0.487 f1 average: 0.709 accuracy: 0.877\n",
      "train BCE loss:  0.26659852266311646  f1 score: 0.639 f1 average: 0.791 accuracy: 0.9\n",
      "validation BCE loss:  0.3311711251735687  f1 score: 0.482 f1 average: 0.706 accuracy: 0.876\n",
      "train BCE loss:  0.2626930773258209  f1 score: 0.649 f1 average: 0.796 accuracy: 0.902\n",
      "validation BCE loss:  0.33071911334991455  f1 score: 0.474 f1 average: 0.701 accuracy: 0.874\n",
      "train BCE loss:  0.2588737905025482  f1 score: 0.653 f1 average: 0.798 accuracy: 0.903\n",
      "validation BCE loss:  0.3290541470050812  f1 score: 0.492 f1 average: 0.711 accuracy: 0.878\n",
      "train BCE loss:  0.25568535923957825  f1 score: 0.662 f1 average: 0.803 accuracy: 0.905\n",
      "validation BCE loss:  0.32771846652030945  f1 score: 0.51 f1 average: 0.72 accuracy: 0.878\n",
      "train BCE loss:  0.2506653070449829  f1 score: 0.673 f1 average: 0.81 accuracy: 0.908\n",
      "validation BCE loss:  0.3294130265712738  f1 score: 0.514 f1 average: 0.722 accuracy: 0.878\n",
      "train BCE loss:  0.24741408228874207  f1 score: 0.678 f1 average: 0.812 accuracy: 0.909\n",
      "validation BCE loss:  0.3270433247089386  f1 score: 0.511 f1 average: 0.721 accuracy: 0.881\n",
      "train BCE loss:  0.24407242238521576  f1 score: 0.686 f1 average: 0.817 accuracy: 0.91\n",
      "validation BCE loss:  0.32899534702301025  f1 score: 0.519 f1 average: 0.725 accuracy: 0.879\n",
      "train BCE loss:  0.24196046590805054  f1 score: 0.691 f1 average: 0.819 accuracy: 0.911\n",
      "validation BCE loss:  0.3273533880710602  f1 score: 0.527 f1 average: 0.73 accuracy: 0.883\n",
      "train BCE loss:  0.23793619871139526  f1 score: 0.691 f1 average: 0.82 accuracy: 0.911\n",
      "validation BCE loss:  0.3266620635986328  f1 score: 0.527 f1 average: 0.729 accuracy: 0.879\n",
      "train BCE loss:  0.23526394367218018  f1 score: 0.707 f1 average: 0.829 accuracy: 0.915\n",
      "validation BCE loss:  0.3263159990310669  f1 score: 0.524 f1 average: 0.728 accuracy: 0.88\n",
      "train BCE loss:  0.23229089379310608  f1 score: 0.709 f1 average: 0.83 accuracy: 0.915\n",
      "validation BCE loss:  0.32622581720352173  f1 score: 0.53 f1 average: 0.73 accuracy: 0.879\n",
      "train BCE loss:  0.22887568175792694  f1 score: 0.717 f1 average: 0.834 accuracy: 0.918\n",
      "validation BCE loss:  0.3262054920196533  f1 score: 0.532 f1 average: 0.732 accuracy: 0.882\n",
      "train BCE loss:  0.22672876715660095  f1 score: 0.721 f1 average: 0.836 accuracy: 0.918\n",
      "validation BCE loss:  0.32809945940971375  f1 score: 0.54 f1 average: 0.736 accuracy: 0.881\n",
      "train BCE loss:  0.22361773252487183  f1 score: 0.716 f1 average: 0.834 accuracy: 0.917\n",
      "validation BCE loss:  0.3282982409000397  f1 score: 0.53 f1 average: 0.73 accuracy: 0.88\n",
      "train BCE loss:  0.22311891615390778  f1 score: 0.727 f1 average: 0.84 accuracy: 0.92\n",
      "validation BCE loss:  0.3271700143814087  f1 score: 0.552 f1 average: 0.742 accuracy: 0.883\n",
      "train BCE loss:  0.21822598576545715  f1 score: 0.735 f1 average: 0.845 accuracy: 0.922\n",
      "validation BCE loss:  0.3315095603466034  f1 score: 0.526 f1 average: 0.728 accuracy: 0.879\n",
      "train BCE loss:  0.2183675616979599  f1 score: 0.729 f1 average: 0.841 accuracy: 0.92\n",
      "validation BCE loss:  0.33053550124168396  f1 score: 0.551 f1 average: 0.742 accuracy: 0.882\n",
      "train BCE loss:  0.2142724096775055  f1 score: 0.743 f1 average: 0.849 accuracy: 0.924\n",
      "validation BCE loss:  0.3296458423137665  f1 score: 0.543 f1 average: 0.737 accuracy: 0.88\n",
      "train BCE loss:  0.21216782927513123  f1 score: 0.739 f1 average: 0.847 accuracy: 0.923\n",
      "validation BCE loss:  0.33578845858573914  f1 score: 0.541 f1 average: 0.736 accuracy: 0.88\n",
      "train BCE loss:  0.21023739874362946  f1 score: 0.747 f1 average: 0.852 accuracy: 0.925\n",
      "validation BCE loss:  0.33555135130882263  f1 score: 0.539 f1 average: 0.736 accuracy: 0.882\n",
      "train BCE loss:  0.20949707925319672  f1 score: 0.75 f1 average: 0.853 accuracy: 0.925\n",
      "validation BCE loss:  0.33178937435150146  f1 score: 0.54 f1 average: 0.735 accuracy: 0.877\n",
      "train BCE loss:  0.20602412521839142  f1 score: 0.75 f1 average: 0.853 accuracy: 0.925\n",
      "validation BCE loss:  0.3384593725204468  f1 score: 0.527 f1 average: 0.728 accuracy: 0.877\n",
      "train BCE loss:  0.203754723072052  f1 score: 0.755 f1 average: 0.856 accuracy: 0.926\n",
      "validation BCE loss:  0.3328890800476074  f1 score: 0.553 f1 average: 0.742 accuracy: 0.88\n",
      "train BCE loss:  0.20195479691028595  f1 score: 0.759 f1 average: 0.858 accuracy: 0.927\n",
      "validation BCE loss:  0.33225131034851074  f1 score: 0.544 f1 average: 0.737 accuracy: 0.879\n",
      "train BCE loss:  0.20088687539100647  f1 score: 0.763 f1 average: 0.86 accuracy: 0.928\n",
      "validation BCE loss:  0.34266725182533264  f1 score: 0.541 f1 average: 0.736 accuracy: 0.879\n",
      "train BCE loss:  0.19847071170806885  f1 score: 0.765 f1 average: 0.862 accuracy: 0.929\n",
      "validation BCE loss:  0.3368336260318756  f1 score: 0.55 f1 average: 0.741 accuracy: 0.881\n",
      "train BCE loss:  0.19652137160301208  f1 score: 0.768 f1 average: 0.863 accuracy: 0.93\n",
      "validation BCE loss:  0.3327743113040924  f1 score: 0.555 f1 average: 0.743 accuracy: 0.88\n",
      "train BCE loss:  0.19465002417564392  f1 score: 0.768 f1 average: 0.864 accuracy: 0.93\n",
      "validation BCE loss:  0.3362712562084198  f1 score: 0.546 f1 average: 0.738 accuracy: 0.879\n",
      "train BCE loss:  0.1943972110748291  f1 score: 0.772 f1 average: 0.866 accuracy: 0.931\n",
      "validation BCE loss:  0.3405275046825409  f1 score: 0.556 f1 average: 0.743 accuracy: 0.879\n",
      "train BCE loss:  0.19183537364006042  f1 score: 0.775 f1 average: 0.867 accuracy: 0.932\n",
      "validation BCE loss:  0.3409513533115387  f1 score: 0.555 f1 average: 0.743 accuracy: 0.882\n",
      "train BCE loss:  0.19042016565799713  f1 score: 0.773 f1 average: 0.866 accuracy: 0.931\n",
      "validation BCE loss:  0.34200021624565125  f1 score: 0.552 f1 average: 0.741 accuracy: 0.879\n",
      "train BCE loss:  0.1873631328344345  f1 score: 0.781 f1 average: 0.871 accuracy: 0.933\n",
      "validation BCE loss:  0.3401538133621216  f1 score: 0.542 f1 average: 0.735 accuracy: 0.875\n",
      "train BCE loss:  0.18629778921604156  f1 score: 0.786 f1 average: 0.874 accuracy: 0.935\n",
      "validation BCE loss:  0.3441935181617737  f1 score: 0.546 f1 average: 0.738 accuracy: 0.878\n",
      "train BCE loss:  0.18542586266994476  f1 score: 0.78 f1 average: 0.87 accuracy: 0.933\n",
      "validation BCE loss:  0.3410901725292206  f1 score: 0.559 f1 average: 0.745 accuracy: 0.88\n",
      "train BCE loss:  0.18297570943832397  f1 score: 0.786 f1 average: 0.874 accuracy: 0.934\n",
      "validation BCE loss:  0.341675341129303  f1 score: 0.559 f1 average: 0.745 accuracy: 0.88\n",
      "train BCE loss:  0.181440532207489  f1 score: 0.793 f1 average: 0.878 accuracy: 0.936\n",
      "validation BCE loss:  0.3454720377922058  f1 score: 0.557 f1 average: 0.745 accuracy: 0.881\n",
      "train BCE loss:  0.17888802289962769  f1 score: 0.801 f1 average: 0.882 accuracy: 0.939\n",
      "validation BCE loss:  0.34277912974357605  f1 score: 0.56 f1 average: 0.745 accuracy: 0.879\n",
      "train BCE loss:  0.17800314724445343  f1 score: 0.8 f1 average: 0.882 accuracy: 0.939\n",
      "validation BCE loss:  0.3515433371067047  f1 score: 0.546 f1 average: 0.738 accuracy: 0.879\n",
      "train BCE loss:  0.17518369853496552  f1 score: 0.798 f1 average: 0.88 accuracy: 0.938\n",
      "validation BCE loss:  0.34802255034446716  f1 score: 0.553 f1 average: 0.741 accuracy: 0.877\n",
      "train BCE loss:  0.17522497475147247  f1 score: 0.801 f1 average: 0.882 accuracy: 0.939\n",
      "validation BCE loss:  0.35644659399986267  f1 score: 0.558 f1 average: 0.744 accuracy: 0.879\n",
      "train BCE loss:  0.1747419536113739  f1 score: 0.803 f1 average: 0.884 accuracy: 0.939\n",
      "validation BCE loss:  0.35188060998916626  f1 score: 0.558 f1 average: 0.744 accuracy: 0.878\n",
      "train BCE loss:  0.17347829043865204  f1 score: 0.802 f1 average: 0.883 accuracy: 0.939\n",
      "validation BCE loss:  0.3525938391685486  f1 score: 0.545 f1 average: 0.737 accuracy: 0.877\n",
      "train BCE loss:  0.17137067019939423  f1 score: 0.807 f1 average: 0.886 accuracy: 0.94\n",
      "validation BCE loss:  0.3532380163669586  f1 score: 0.551 f1 average: 0.74 accuracy: 0.877\n",
      "train BCE loss:  0.1706576645374298  f1 score: 0.81 f1 average: 0.887 accuracy: 0.941\n",
      "validation BCE loss:  0.3506928086280823  f1 score: 0.565 f1 average: 0.748 accuracy: 0.88\n",
      "train BCE loss:  0.1678847074508667  f1 score: 0.814 f1 average: 0.89 accuracy: 0.942\n",
      "validation BCE loss:  0.35652628540992737  f1 score: 0.559 f1 average: 0.744 accuracy: 0.877\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "using adagrad because it assign bigger updates to less frequently updated weights \n",
    "(like words that are not used many times)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "optimizer = optim.Adagrad(myTransformer.parameters(),lr = 0.001)\n",
    "\n",
    "for i in range(100):\n",
    "  trainpreds = torch.tensor([])\n",
    "  traintrues = torch.tensor([])\n",
    "  for  batch in train_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    myTransformer.zero_grad()\n",
    "    pred = myTransformer(X).squeeze()\n",
    "    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
    "    traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
    "    err = F.binary_cross_entropy(pred,y)\n",
    "    err.backward()\n",
    "    optimizer.step()\n",
    "  err = F.binary_cross_entropy(trainpreds,traintrues)\n",
    "  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n",
    " \n",
    "\n",
    "  valpreds = torch.tensor([])\n",
    "  valtrues = torch.tensor([])\n",
    "  for batch in valid_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
    "    pred = myTransformer(X).squeeze().cpu().detach()\n",
    "    # print(valtrues.shape)\n",
    "    valpreds = torch.cat((valpreds,pred))\n",
    "  err = F.binary_cross_entropy(valpreds,valtrues)\n",
    "  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.065342,
     "end_time": "2022-12-09T01:37:15.800469",
     "exception": false,
     "start_time": "2022-12-09T01:37:15.735127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def saveVocab(vocab, file):\n",
    "    path = Path(file).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Saving vocab file \" + vocab.__str__() + \" --> \" + file)\n",
    "    torch.save(vocab, file)so the final scores on validation are  \n",
    "\n",
    "validation BCE loss:  0.137 f1 score: 0.706 f1 average: 0.84 accuracy: 0.952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:15.947705Z",
     "iopub.status.busy": "2022-12-09T01:37:15.946889Z",
     "iopub.status.idle": "2022-12-09T01:37:16.482694Z",
     "shell.execute_reply": "2022-12-09T01:37:16.482038Z"
    },
    "id": "vordglMh4GGC",
    "outputId": "b65b56fa-c86c-449d-8c97-83e479f537fa",
    "papermill": {
     "duration": 0.615753,
     "end_time": "2022-12-09T01:37:16.482832",
     "exception": false,
     "start_time": "2022-12-09T01:37:15.867079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test BCE loss:  0.3518851399421692  f1 score: 0.579 f1 average: 0.755 accuracy: 0.881\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "now getting the results on the test set\n",
    "\"\"\"\n",
    "\n",
    "testpreds = torch.tensor([])\n",
    "testtrues = torch.tensor([])\n",
    "for batch in test_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    testtrues = torch.cat((testtrues,y.cpu().detach()))\n",
    "    pred = myTransformer(X).squeeze().cpu().detach()\n",
    "    testpreds = torch.cat((testpreds,pred))\n",
    "err = F.binary_cross_entropy(testpreds,testtrues)\n",
    "print(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:16.622822Z",
     "iopub.status.busy": "2022-12-09T01:37:16.622040Z",
     "iopub.status.idle": "2022-12-09T01:37:16.636518Z",
     "shell.execute_reply": "2022-12-09T01:37:16.636075Z"
    },
    "papermill": {
     "duration": 0.085761,
     "end_time": "2022-12-09T01:37:16.636617",
     "exception": false,
     "start_time": "2022-12-09T01:37:16.550856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16729</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @O3_Millz: Black pussy remind me of roast b...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12862</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Mississippi state fans are the best to just mo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10474</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate that wee boy wae the squinty eyes in th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11467</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Idgaf if it's 3pm in the afternoon bitch make ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23295</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>You drink light beer? Hahahahahahahahahahaha w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "16729      3            0                   3        0      1   \n",
       "12862      3            0                   1        2      2   \n",
       "10474      3            1                   0        2      2   \n",
       "11467      3            1                   2        0      1   \n",
       "23295      3            0                   2        1      1   \n",
       "\n",
       "                                                   tweet  predicted  \n",
       "16729  RT @O3_Millz: Black pussy remind me of roast b...        1.0  \n",
       "12862  Mississippi state fans are the best to just mo...        1.0  \n",
       "10474  I hate that wee boy wae the squinty eyes in th...        1.0  \n",
       "11467  Idgaf if it's 3pm in the afternoon bitch make ...        1.0  \n",
       "23295  You drink light beer? Hahahahahahahahahahaha w...        1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"predicted\"] = torch.round(testpreds).numpy()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this shows that the model understands the language well \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test[test.predicted==1].iloc[32:37]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:16.777275Z",
     "iopub.status.busy": "2022-12-09T01:37:16.776771Z",
     "iopub.status.idle": "2022-12-09T01:37:16.800064Z",
     "shell.execute_reply": "2022-12-09T01:37:16.799445Z"
    },
    "papermill": {
     "duration": 0.096014,
     "end_time": "2022-12-09T01:37:16.800190",
     "exception": false,
     "start_time": "2022-12-09T01:37:16.704176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = TEXT.process([\"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\"])\n",
    "x = x.to(device)\n",
    "pred = myTransformer(x).squeeze().cpu().detach()\n",
    "print(torch.round(pred))\n",
    "\n",
    "x = TEXT.process([\"!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;\"])\n",
    "x = x.to(device)\n",
    "pred = myTransformer(x).squeeze().cpu().detach()\n",
    "print(torch.round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:16.941039Z",
     "iopub.status.busy": "2022-12-09T01:37:16.940315Z",
     "iopub.status.idle": "2022-12-09T01:37:16.943139Z",
     "shell.execute_reply": "2022-12-09T01:37:16.942748Z"
    },
    "papermill": {
     "duration": 0.074804,
     "end_time": "2022-12-09T01:37:16.943232",
     "exception": false,
     "start_time": "2022-12-09T01:37:16.868428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def saveVocab(vocab, file):\n",
    "    path = Path(file).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Saving vocab file \" + vocab.__str__() + \" --> \" + file)\n",
    "    torch.save(vocab, file, pickle_module=dill)\n",
    "    \n",
    "# def saveVocab(field, file):\n",
    "#     path = Path(file).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "#     with open(file, 'w+', encoding='utf-8') as f:     \n",
    "#         for token, index in field.vocab.stoi.items():\n",
    "#             print (token, len(token),index)\n",
    "#             f.write(f'{index}\\t{token}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:17.084514Z",
     "iopub.status.busy": "2022-12-09T01:37:17.083804Z",
     "iopub.status.idle": "2022-12-09T01:37:17.086240Z",
     "shell.execute_reply": "2022-12-09T01:37:17.086645Z"
    },
    "papermill": {
     "duration": 0.075634,
     "end_time": "2022-12-09T01:37:17.086769",
     "exception": false,
     "start_time": "2022-12-09T01:37:17.011135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadVocab(file):\n",
    "    if os.path.isfile(file):\n",
    "        print(\"Reading vocab file: \" + file)\n",
    "        return torch.load(file, pickle_module=dill)\n",
    "    else:\n",
    "        print(\"Error reading file: \" + filee + \". file do not exist.\")\n",
    "\n",
    "# def loadVocab(file):\n",
    "#     if os.path.isfile(file):\n",
    "#         vocab_dict = dict()\n",
    "#         field = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\n",
    "        \n",
    "#         with open(file, 'r', encoding='utf-8') as f:\n",
    "#             for line in f:\n",
    "#                 print(line)\n",
    "#                 index, token = line.split('\\t')\n",
    "#                 vocab_dict[token] = int(index)\n",
    "#         field.vocab = vocab_dict\n",
    "#         return field\n",
    "#     else:\n",
    "#         print(\"Error reading file: \" + file + \". file do not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:17.229239Z",
     "iopub.status.busy": "2022-12-09T01:37:17.228617Z",
     "iopub.status.idle": "2022-12-09T01:37:17.231762Z",
     "shell.execute_reply": "2022-12-09T01:37:17.231317Z"
    },
    "papermill": {
     "duration": 0.076403,
     "end_time": "2022-12-09T01:37:17.231860",
     "exception": false,
     "start_time": "2022-12-09T01:37:17.155457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def saveModel(model, file):\n",
    "    path = Path(file).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Saving vocab file \" + model.__str__() + \" --> \" + file)\n",
    "    torch.save(model.state_dict(), file, pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:17.374380Z",
     "iopub.status.busy": "2022-12-09T01:37:17.373639Z",
     "iopub.status.idle": "2022-12-09T01:37:17.376422Z",
     "shell.execute_reply": "2022-12-09T01:37:17.376001Z"
    },
    "papermill": {
     "duration": 0.076774,
     "end_time": "2022-12-09T01:37:17.376528",
     "exception": false,
     "start_time": "2022-12-09T01:37:17.299754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadModel(file):\n",
    "    if os.path.isfile(file):\n",
    "        print(\"Reading model file: \" + file)\n",
    "        model = TextTransformer()\n",
    "        model.load_state_dict(torch.load(file, pickle_module=dill))\n",
    "        return model\n",
    "    else:\n",
    "        print(\"Error reading file: \" + file + \". file do not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:17.516906Z",
     "iopub.status.busy": "2022-12-09T01:37:17.516114Z",
     "iopub.status.idle": "2022-12-09T01:37:17.800883Z",
     "shell.execute_reply": "2022-12-09T01:37:17.801519Z"
    },
    "papermill": {
     "duration": 0.357029,
     "end_time": "2022-12-09T01:37:17.801689",
     "exception": false,
     "start_time": "2022-12-09T01:37:17.444660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocab file <torchtext.data.field.Field object at 0x7fb0334472d0> --> /kaggle/working/vocab/TEXT_obj.pth\n"
     ]
    }
   ],
   "source": [
    "saveVocab(TEXT, \"/kaggle/working/vocab/TEXT_obj.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:17.945049Z",
     "iopub.status.busy": "2022-12-09T01:37:17.944176Z",
     "iopub.status.idle": "2022-12-09T01:37:17.959230Z",
     "shell.execute_reply": "2022-12-09T01:37:17.959992Z"
    },
    "papermill": {
     "duration": 0.088978,
     "end_time": "2022-12-09T01:37:17.960164",
     "exception": false,
     "start_time": "2022-12-09T01:37:17.871186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading vocab file: /kaggle/working/vocab/TEXT_obj.pth\n"
     ]
    }
   ],
   "source": [
    "NEW_TEXT = loadVocab(\"/kaggle/working/vocab/TEXT_obj.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:18.122271Z",
     "iopub.status.busy": "2022-12-09T01:37:18.121312Z",
     "iopub.status.idle": "2022-12-09T01:37:18.140554Z",
     "shell.execute_reply": "2022-12-09T01:37:18.139954Z"
    },
    "papermill": {
     "duration": 0.106807,
     "end_time": "2022-12-09T01:37:18.140671",
     "exception": false,
     "start_time": "2022-12-09T01:37:18.033864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocab file TextTransformer(\n",
      "  (wordEmbeddings): Embedding(4900, 140)\n",
      "  (positionEmbeddings): Embedding(140, 20)\n",
      "  (transformerLayer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): Linear(in_features=160, out_features=160, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "    (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=160, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (linear3): Linear(in_features=140, out_features=16, bias=True)\n",
      "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
      ") --> /kaggle/working/model/textTransformer_states.pth\n"
     ]
    }
   ],
   "source": [
    "saveModel(myTransformer, \"/kaggle/working/model/textTransformer_states.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:18.284167Z",
     "iopub.status.busy": "2022-12-09T01:37:18.283512Z",
     "iopub.status.idle": "2022-12-09T01:37:18.308010Z",
     "shell.execute_reply": "2022-12-09T01:37:18.308622Z"
    },
    "papermill": {
     "duration": 0.098047,
     "end_time": "2022-12-09T01:37:18.308781",
     "exception": false,
     "start_time": "2022-12-09T01:37:18.210734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model file: /kaggle/working/model/textTransformer_states.pth\n"
     ]
    }
   ],
   "source": [
    "newTransformer = loadModel(\"/kaggle/working/model/textTransformer_states.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:18.453932Z",
     "iopub.status.busy": "2022-12-09T01:37:18.453228Z",
     "iopub.status.idle": "2022-12-09T01:37:18.456133Z",
     "shell.execute_reply": "2022-12-09T01:37:18.455671Z"
    },
    "papermill": {
     "duration": 0.076842,
     "end_time": "2022-12-09T01:37:18.456234",
     "exception": false,
     "start_time": "2022-12-09T01:37:18.379392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, vocab, inString, device):\n",
    "    model.eval() #switching to evaluation mode.\n",
    "    model.to(device)\n",
    "    x = TEXT.process([inString])\n",
    "    x = x.to(device)\n",
    "    pred = myTransformer(x).squeeze().cpu().detach()\n",
    "    return torch.round(pred).numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:37:18.599937Z",
     "iopub.status.busy": "2022-12-09T01:37:18.598929Z",
     "iopub.status.idle": "2022-12-09T01:37:18.610090Z",
     "shell.execute_reply": "2022-12-09T01:37:18.610741Z"
    },
    "papermill": {
     "duration": 0.085549,
     "end_time": "2022-12-09T01:37:18.610903",
     "exception": false,
     "start_time": "2022-12-09T01:37:18.525354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(inference(newTransformer, NEW_TEXT, \"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\", device))\n",
    "print(inference(newTransformer, NEW_TEXT, \"!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;\", device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 494.31695,
   "end_time": "2022-12-09T01:37:18.890511",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-09T01:29:04.573561",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
